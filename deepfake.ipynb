{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMYKEWiwpb_2",
        "outputId": "0c6a2bec-7e29-4035-a216-fb1f833014dc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from mtcnn) (1.5.2)\n",
            "Requirement already satisfied: lz4>=4.3.3 in /usr/local/lib/python3.12/dist-packages (from mtcnn) (4.4.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install mtcnn\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-geometric\n",
        "!pip install opencv-python\n",
        "\n",
        "#packages\n",
        "import zipfile, os, shutil, cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch_geometric.data import Data, Batch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.nn import GCNConv, global_max_pool\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"/content/split_frames_dataset.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n",
        "\n",
        "# check dataset folder exists\n",
        "print(os.listdir(\"/content\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEKvqa_PYXmr",
        "outputId": "e9b7e68c-4ec9-4b98-d7ff-06c2d1db3454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'split_frames_dataset.zip', 'split_frames_dataset', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "def load_image_paths(base_dir, label):\n",
        "    # Recursively grab all jpg frames from person folders\n",
        "    files = glob.glob(os.path.join(base_dir, \"**\", \"*.jpg\"), recursive=True)\n",
        "    return [(f, label) for f in files]\n",
        "\n",
        "# Train\n",
        "real_train_files = load_image_paths(\"/content/split_frames_dataset/train/real\", 0)\n",
        "fake_train_files = load_image_paths(\"/content/split_frames_dataset/train/fake\", 1)\n",
        "\n",
        "# Test\n",
        "real_val_files = load_image_paths(\"/content/split_frames_dataset/test/real\", 0)\n",
        "fake_val_files = load_image_paths(\"/content/split_frames_dataset/test/fake\", 1)\n",
        "\n",
        "print(\"✅ Train Real:\", len(real_train_files))\n",
        "print(\"✅ Train Fake:\", len(fake_train_files))\n",
        "print(\"✅ Test Real:\", len(real_val_files))\n",
        "print(\"✅ Test Fake:\", len(fake_val_files))"
      ],
      "metadata": {
        "id": "FO3nlSjD-ljP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e5e7f7-3064-4b54-c753-67708b7d7c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Train Real: 166\n",
            "✅ Train Fake: 166\n",
            "✅ Test Real: 42\n",
            "✅ Test Fake: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_graph(image_tensor, k=9, patch_size=32, debug=True):\n",
        "    \"\"\"\n",
        "    Converts an image tensor [3,H,W] into a graph where\n",
        "    each node = flattened patch of size (3*patch_size*patch_size).\n",
        "    \"\"\"\n",
        "    C, H, W = image_tensor.shape\n",
        "    if H < patch_size or W < patch_size:\n",
        "        if debug:\n",
        "            print(f\"⚠️ Skipping tiny frame: {image_tensor.shape}\")\n",
        "        return None\n",
        "\n",
        "    # Extract patches: [C, num_patches_h, num_patches_w, ps, ps]\n",
        "    patches = image_tensor.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
        "    patches = patches.permute(1, 2, 0, 3, 4).contiguous()   # [num_patches_h, num_patches_w, C, ps, ps]\n",
        "    patches = patches.view(-1, C * patch_size * patch_size)  # [num_patches, 3072]\n",
        "\n",
        "    if patches.size(0) < 2:\n",
        "        if debug:\n",
        "            print(f\"⚠️ Too few patches: {patches.shape}\")\n",
        "        return None\n",
        "\n",
        "    #if debug:\n",
        "       # print(f\"✅ Graph created: nodes={patches.size(0)}, features={patches.size(1)}\")\n",
        "\n",
        "    # Build similarity graph\n",
        "    similarity = cosine_similarity(patches.cpu().numpy())\n",
        "    edge_index = []\n",
        "    for i in range(len(patches)):\n",
        "        indices = similarity[i].argsort()[-k-1:-1]\n",
        "        edge_index += [(i, j) for j in indices]\n",
        "\n",
        "    if len(edge_index) == 0:\n",
        "        if debug:\n",
        "            print(\"⚠️ No edges created\")\n",
        "        return None\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    x = patches.float()  # [num_patches, 3072]\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index)"
      ],
      "metadata": {
        "id": "iFruFALm-C39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceFusionDataset(Dataset):\n",
        "    def __init__(self, file_list):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((128, 128)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.file_list[idx]\n",
        "\n",
        "        # Try up to N attempts in case of bad frames\n",
        "        attempts = 0\n",
        "        while attempts < len(self.file_list):\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None:\n",
        "                idx = (idx + 1) % len(self.file_list)\n",
        "                attempts += 1\n",
        "                continue\n",
        "\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image_tensor = self.transform(Image.fromarray(image))\n",
        "\n",
        "            graph = image_to_graph(image_tensor)\n",
        "            if graph is not None:\n",
        "                graph.y = torch.tensor(label, dtype=torch.long)\n",
        "                return image_tensor, graph\n",
        "\n",
        "            # If graph failed, move to next image\n",
        "            idx = (idx + 1) % len(self.file_list)\n",
        "            attempts += 1\n",
        "\n",
        "        # If all fail\n",
        "        raise RuntimeError(\"❌ All images failed to generate a valid graph.\")\n"
      ],
      "metadata": {
        "id": "EI2O1GvT3YJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4PvEb14pR58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model Architectures\n",
        "import torchvision.models as models\n",
        "\n",
        "class CNNBranch(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    base = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "    self.feature_extractor = nn.Sequential(*list(base.children())[:-1])\n",
        "    self.fc = nn.Linear(512, 600)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.feature_extractor(x).view(x.size(0),-1)\n",
        "    return self.fc(x)\n",
        "\n",
        "class HierarchicalGNNBranch(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fds = [80,160,400,600]\n",
        "    self.mlps = nn.ModuleList([\n",
        "        nn.Sequential(\n",
        "            nn.Linear(3072 if i==0 else self.fds[i-1],fd*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.6),\n",
        "            nn.Linear(fd*4,fd),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.6)\n",
        "        ) for i,fd in enumerate(self.fds)\n",
        "    ])\n",
        "    self.convs = nn.ModuleList([GCNConv(fd,fd) for fd in self.fds])\n",
        "    self.norms = nn.ModuleList([nn.BatchNorm1d(fd) for fd in self.fds])\n",
        "    self.dropout = nn.Dropout(0.6)\n",
        "\n",
        "  def forward(self,data):\n",
        "    x,edge_index,batch = data.x, data.edge_index, data.batch\n",
        "    for i in range(len(self.fds)):\n",
        "      x_res = x # Store the output of the previous layer before applying current MLP\n",
        "      x = self.mlps[i](x)\n",
        "      x = self.convs[i](x,edge_index)\n",
        "      x = self.norms[i](x)\n",
        "      if i > 0 and x.size(1) == x_res.size(1): # Apply residual connection only after the first layer AND if feature sizes match\n",
        "          x = F.relu(x+x_res)\n",
        "      else:\n",
        "          x = F.relu(x) # Just apply ReLU otherwise\n",
        "      x = self.dropout(x)\n",
        "    return global_max_pool(x,batch)\n",
        "\n",
        "class FuNetA(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.cnn = CNNBranch()\n",
        "    self.gnn = HierarchicalGNNBranch()\n",
        "    self.fc = nn.Linear(600,2)\n",
        "  def forward(self,img,graph):\n",
        "    return self.fc(self.cnn(img)+self.gnn(graph))\n",
        "\n",
        "class FuNetM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.cnn = CNNBranch()\n",
        "    self.gnn = HierarchicalGNNBranch()\n",
        "    self.fc = nn.Linear(600,2)\n",
        "  def forward(self,img,graph):\n",
        "    return self.fc(self.cnn(img)*self.gnn(graph))\n",
        "\n",
        "\n",
        "class FuNetC(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.cnn = CNNBranch()\n",
        "    self.gnn = HierarchicalGNNBranch()\n",
        "    self.fc = nn.Linear(1200,2)\n",
        "  def forward(self,img,graph):\n",
        "    return self.fc(torch.cat([self.cnn(img),self.gnn(graph)],dim=1))"
      ],
      "metadata": {
        "id": "w6z2wzNV_A9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Batch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Build train/val datasets\n",
        "train_set = FaceFusionDataset(real_train_files + fake_train_files)\n",
        "val_set   = FaceFusionDataset(real_val_files + fake_val_files)\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]  # filter out None\n",
        "    if len(batch) == 0:\n",
        "        return None, None\n",
        "    images = torch.stack([b[0] for b in batch])\n",
        "    graphs = Batch.from_data_list([b[1] for b in batch])\n",
        "    return images, graphs\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_set, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_set, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(\"✅ Train samples:\", len(train_set))\n",
        "print(\"✅ Val samples:\", len(val_set))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRfVo1yScFOp",
        "outputId": "a586ee97-df7f-4c0f-8a0f-6083edc40a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Train samples: 332\n",
            "✅ Val samples: 84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, graphs in loader:\n",
        "        if images is None:\n",
        "            continue\n",
        "        images, graphs = images.to(device), graphs.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(images, graphs)\n",
        "        loss = F.cross_entropy(out, graphs.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, graphs in loader:\n",
        "            images, graphs = images.to(device), graphs.to(device)\n",
        "            out = model(images, graphs)\n",
        "            loss = F.cross_entropy(out, graphs.y)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred, y_prob = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for images, graphs in loader:\n",
        "            images, graphs = images.to(device), graphs.to(device)\n",
        "            logits = model(images, graphs)\n",
        "            probs = F.softmax(logits, dim=1)[:, 1]\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            y_true.extend(graphs.y.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "            y_prob.extend(probs.cpu().numpy())\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred),\n",
        "        'recall': recall_score(y_true, y_pred),\n",
        "        'f1': f1_score(y_true, y_pred),\n",
        "        'auc': roc_auc_score(y_true, y_prob)\n",
        "    }\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0.0001):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if val_loss < self.best_loss - self.delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.best_model_state = model.state_dict()\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ],
      "metadata": {
        "id": "2zmBtn64cw7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop + validation loop + results\n",
        "models={\n",
        "    'FuNet-A':FuNetA(),\n",
        "    #'FuNet-M':FuNetM(),\n",
        "    #'FuNet-C':FuNetC()\n",
        "\n",
        "}\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_histories={}\n",
        "results={}\n",
        "for name,model in models.items():\n",
        "  model=model.to(device)\n",
        "  optimizer=torch.optim.AdamW(model.parameters(),lr=2e-4)\n",
        "  early_stopper=EarlyStopping(patience=5)\n",
        "\n",
        "  train_losses=[]\n",
        "  val_losses=[]\n",
        "  for epoch in range(1,3):\n",
        "    train_loss=train(model,train_loader,optimizer,device)\n",
        "    val_loss=validate(model,val_loader,device)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    early_stopper(val_loss,model)\n",
        "    print(f\"Epoch:{epoch} : Train Loss={train_loss:.4f}, Validation Loss={val_loss:.4f}\")\n",
        "    if early_stopper.early_stop:\n",
        "      print(\"Early stopping\")\n",
        "      break\n",
        "\n",
        "model.load_state_dict(early_stopper.best_model_state)\n",
        "model_histories[name]={\n",
        "    'train_losses':train_losses,\n",
        "    'val_losses':val_losses\n",
        "}\n",
        "results[name]=evaluate(model,val_loader,device)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n6SXWLls_jkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d802f6-31b7-4ce5-de10-792142cc2361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1 : Train Loss=0.7040, Validation Loss=0.4438\n",
            "Epoch:2 : Train Loss=0.2771, Validation Loss=0.0771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEVXn1WwnuRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model.state_dict(), \"funet_a_full.pth\")\n",
        "\n",
        "print(\"Funet A model saved successfully!\")"
      ],
      "metadata": {
        "id": "gtrontupKzyB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a46ce8c-5e30-4318-c9b4-417351def07b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funet A model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "files.download(\"funet_a_full.pth\")\n"
      ],
      "metadata": {
        "id": "9PcisBxTMxI5",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "924ca6ad-90be-47c7-9ddc-5cbd6bc3d5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a4976b5c-8701-4133-b536-3ba6f298b38f\", \"funet_a_full.pth\", 66129365)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}